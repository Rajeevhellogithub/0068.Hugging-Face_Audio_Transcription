{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dc21a9-1954-4742-9d6d-452ad0dd2df4",
   "metadata": {},
   "source": [
    "**Do this Practical in Google Colab GPU**\n",
    "\n",
    "https://huggingface.co/openai/whisper-small  \n",
    "https://learn.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development  \n",
    "https://huggingface.co/docs/huggingface_hub/v0.27.0/guides/manage-cache#limitations  \n",
    "https://github.com/mohan696matlab/Gen-AI-Mini-Projects/blob/main/audio2text_from_mp3/audio2text.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d3ccf5-8738-4e1e-9e23-e6b89ab7e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51d517f-f7f2-458e-a3cb-da6bd96d3cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8ab5c23ed34d179f58fc56cd24f7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PYTHONCLASSTF\\tensorflowvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RAJEEV\\.cache\\huggingface\\hub\\models--openai--whisper-tiny. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9c49772121459e8499ce05278bb48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2186d7b9954d70b933d6370ed559d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de77c657b4ae473d80ab2d8ccba3ff2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154feca280ea4b2ca1e48e3307513bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0594bb0147c84dc1a1f9e6e814d0ed1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc75406191324b3791c89eba360a30fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a737b9054d247e996b547e061f5a785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c9ed9b29bb4b1c853e2f5707603c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9d45d4e432464587abb6829da2fa68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fae931e296f40cc802b239e57aaec76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pipe  = pipeline(\"automatic-speech-recognition\",\n",
    "                    \"openai/whisper-tiny\", \n",
    "                    chunk_length_s=30,\n",
    "                    stride_length_s=5,\n",
    "                    return_timestamps=True,\n",
    "                    device=device, \n",
    "                    generate_kwargs = {\"language\": 'English', \"task\": \"translate\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "881529b5-0c14-45e8-9bcc-bd375c0f911c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PYTHONCLASSTF\\tensorflowvenv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=translate, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=translate.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 7.28)--> Community Update December 2019 posted on December 27th, 2019 by Frederick Font.\n",
      "(7.28, 10.8)--> Hi everyone, welcome to a new community update.\n",
      "(10.8, 18.36)--> Yeah, we know we have not been updated and you very regularly lately, but this does not mean we have not been working hard on free sound.\n",
      "(18.36, 30.04)--> As has been the case for the last year, we have not been very much concentrated on working on either under the hood improvements or research type of issues which do not have a clearly visible output in the\n",
      "(30.04, 32.24)--> free sound website yet.\n",
      "(32.24, 37.12)--> But we are indeed working on great things which will definitely end up in the platform.\n",
      "(37.12, 41.72)--> Here is a summary of our current main working threads.\n",
      "(41.72, 45.44)--> Bug fixes, general maintenance, and software updates.\n",
      "(45.44, 49.68)--> This is a big one as we are about to carry out necessary software updates.\n",
      "(49.68, 55.28)--> For nerds, Python, Django updates, which affect all of our codebase and are therefore\n",
      "(55.28, 58.2)--> quite time consuming.\n",
      "(58.2, 59.2)--> New features.\n",
      "(59.2, 62.96)--> We are working on new features mostly related to the search page.\n",
      "(62.96, 66.08)--> However, all these new features require a lot of previous\n",
      "(66.08, 72.08)--> research work. Don't forget we're a research institution, so that's what we do best. And again,\n",
      "(72.08, 77.2)--> features need their time to become a reality. The new search features we're working on will\n",
      "(77.2, 86.6)--> allow to cluster search results as well as adding new filtering options. New front end. Yes, we have not abandoned this one.\n",
      "(86.6, 91.64)--> It is going very slowly, much more than we thought, but it will eventually become a reality\n",
      "(91.64, 93.96)--> and it is indeed in our roadmap.\n",
      "(93.96, 98.44)--> Oh, and by the way, we've just published a tech-oriented post about free sound in the\n",
      "(98.44, 101.16)--> creative Commons open source blog.\n",
      "(101.16, 103.12)--> You might want to check that out.\n",
      "(103.12, 105.68)--> And that's it for the short update. Thanks for\n",
      "(105.68, 111.12)--> reading and stay tuned for the updates in the coming year. Big happy new year to everyone.\n"
     ]
    }
   ],
   "source": [
    "transcription = pipe(r\"E:\\PYTHONCLASSTF\\PrakashSenapati\\2024_12_20_Audio_Processing\\audio1.mp3\" )\n",
    "\n",
    "formatted_lyrics = \"\"\n",
    "for line in transcription['chunks']:\n",
    "    text = line[\"text\"]\n",
    "    ts = line[\"timestamp\"]\n",
    "    formatted_lyrics += f\"{ts}-->{text}\\n\"\n",
    "\n",
    "print(formatted_lyrics.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e3ff67-d328-457c-bf08-e6b62329fd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1202c7da67340c093d72cb606f6713b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea43896702784ad79c9dde6a20262cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d5aebf8108403097808c2ecf97987f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/339 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b2e12506994221a94931c4504de1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/282k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e0c1b46bd845e7be6e0af38d37cd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57fad6b6ba941039cad889a426bdd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.41M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621f46cb98e34c9ba501034fe37c601b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75691644e7a04ffea8a4dbccb13639d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95cad208efc4e4099b9d8fd32aada0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c816ca04894fce9d365b0054caa11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_id = \"distil-whisper/distil-small.en\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    return_timestamps=True,\n",
    "    chunk_length_s=15,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c73e8f-28d4-44e6-a166-a4c1e7edfd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 3.64)--> And after running this you see that for 10 seconds.\n",
      "(3.64, 8.32)--> After every 10 second it will start giving the transcription for what I have said.\n",
      "(8.32, 10.6)--> So here you see the 10 second has passed.\n",
      "(10.6, 11.6)--> Yep.\n",
      "(11.6, 15.4)--> And now you see the first transcription here.\n",
      "(15.4, 20.12)--> After that, like every 10 second, whatever I said, it will start transcribing.\n",
      "(20.12, 23.48)--> But that is one that you don't get the live transcription.\n",
      "(27.84, 29.4)--> You get whatever is transcribed after you know 10 second, 20 second, depending upon your use case.\n",
      "(30.08, 35.92)--> If you want a very real time transcription, you can try to tweak this one like transcription interval\n",
      "(35.92, 40.08)--> you can reduce this to let's say two second five seconds so you see this transcription\n",
      "(40.08, 42.6)--> like after every five second you start seeing a transcription.\n",
      "(42.6, 47.32)--> But there is the problem that you know it suppose you have very small\n",
      "(47.32, 51.4)--> transcription length for example one second or two second then it will start\n",
      "(51.4, 59.32)--> splitting your data like for example after you said in the text that and after running this you see that\n",
      "(59.64, 64.2)--> Which the model takes into account, but during the generation it cannot\n",
      "(64.2, 69.4)--> takes input that the immediate next text so you miss that that is the trade-off\n",
      "(69.4, 79.92)--> you have to you know take care of and, so that was the main idea of this particular video, but I have\n",
      "(79.92, 83.28)--> also created a streamly app for the same case.\n",
      "(83.28, 89.28)--> I'll make it available on get up.\n",
      "(89.28, 92.2)--> The app interface looks something like this.\n",
      "(92.2, 95.12)--> So here you can select your microphone,\n",
      "(95.12, 100.4)--> then the language you want to speak and transcribe or translate. And on the\n",
      "(100.4, 105.8)--> transcription it will do live transcription of whatever you are speaking.\n",
      "(105.8, 109.04)--> So yeah, that was it for this video.\n",
      "(109.04, 115.28)--> And in the upcoming videos, how we can, we'll learn more about this whisper model and how we can fine-tune our own data.\n",
      "(118.48, 127.28)--> How can we find tune whisper model on data? For example there are many languages which are absent from the this whisper model\n",
      "(127.84, 136.8)--> such as low resource language, something like I come from Odisha, so Odia language is missing from the whisper data set. So I would like to\n",
      "(136.8, 143.24)--> fine-tune it on a very low resource data and see how it performs. We evaluate the model's performance\n",
      "(143.24, 154.28)--> and see whether it's able to capture the idea of the new language or not.\n",
      "(154.28, 156.28)--> So yeah, let us sit about this video.\n",
      "(156.28, 157.76)--> We'll see you in the next video.\n"
     ]
    }
   ],
   "source": [
    "transcription = pipe(r\"E:\\PYTHONCLASSTF\\PrakashSenapati\\2024_12_20_Audio_Processing\\audio3.wav\" )\n",
    "\n",
    "formatted_lyrics = \"\"\n",
    "for line in transcription['chunks']:\n",
    "    text = line[\"text\"]\n",
    "    ts = line[\"timestamp\"]\n",
    "    formatted_lyrics += f\"{ts}-->{text}\\n\"\n",
    "\n",
    "print(formatted_lyrics.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowkernel",
   "language": "python",
   "name": "tensorflowkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
